{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.defects import S_cal\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorboard\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from utils.model import DownSampleConv\n",
    "from utils.defects import theta_cal, S_cal\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy\n",
    "from kinetic_solver import KineticSolver, KineticData\n",
    "from nematic_env import ActiveNematicEnv, RewardLoggingCallback\n",
    "\n",
    "\n",
    "# 初始化物理仿真器\n",
    "geo_params = {\n",
    "    'N': 256,\n",
    "    'Nth': 256,\n",
    "    'L': 10\n",
    "}\n",
    "flow_params = {\n",
    "    'dT': 0.3,\n",
    "    'dR': 0.3,\n",
    "    'alpha': -10,\n",
    "    'beta': 1.0,\n",
    "    'zeta': 2,\n",
    "    'V0': 0.0\n",
    "}\n",
    "simu_params = {\n",
    "    'dt': 0.0004,\n",
    "    'seed': 1234,\n",
    "    'inner_steps': 8,\n",
    "    'outer_steps': 6400\n",
    "}\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dx_  0.0390625  dth_  0.02454369260617026  dt  0.0004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kinetic_solver.KineticData object at 0x7c46ff6cc6d0>\n",
      "Pre iteration done.\n"
     ]
    }
   ],
   "source": [
    "solver_paras = (geo_params, flow_params, simu_params)\n",
    "# env = ActiveNematicEnv(solver_paras, device=device)\n",
    "\n",
    "# solver_paras = (geo_params, flow_params, simu_params)\n",
    "\n",
    "solver = KineticSolver(*solver_paras, device=device)\n",
    "data_path = '/home/hou63/pj2/Nematic_RL/datas/simulation_data_test.pkl'\n",
    "# simulation_data = KineticData(*solver.initialize2_pytorch(seed=918), solver.simu_args)\n",
    "simulation_data = KineticData.loader(data_path)\n",
    "# simulation_data = solver.preloop_kinetic(simulation_data, num_itr=32000)\n",
    "print(simulation_data)\n",
    "simulation_data = simulation_data.loader(data_path)\n",
    "env = ActiveNematicEnv(solver_paras, solver=solver,\n",
    "                        simulation_data=simulation_data, device=device,\n",
    "                        data_path=data_path, intensity=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env._calculate_reward())\n",
    "\n",
    "d11, d12 = simulation_data.get_D()\n",
    "S = S_cal(d11.cpu().data.numpy(), d12.cpu().data.numpy())\n",
    "print(S.mean())\n",
    "d11 = d11.cpu().data.numpy()\n",
    "d12 = d12.cpu().data.numpy()\n",
    "theta = 0.5 * np.arctan2(2*d12, 2*d11-1)\n",
    "# plot S\n",
    "\n",
    "cost, sint = np.cos(theta), np.sin(theta)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,3, figsize=(15, 5))\n",
    "ax1[0].streamplot(np.arange(0, theta.shape[0]), np.arange(0, theta.shape[1]),\n",
    "                cost, sint, arrowsize=0.5, color='r',\n",
    "                density=4, linewidth=0.5)\n",
    "\n",
    "ax1[1].imshow(d11)\n",
    "# set colorbar\n",
    "\n",
    "ax1[2].imshow(S)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /home/hou63/pj2/Nematic_RL/logs/PPO_2\n",
      "----------------------------------\n",
      "| step/              |           |\n",
      "|    rewards         | 0.7246866 |\n",
      "| time/              |           |\n",
      "|    fps             | 3         |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 84        |\n",
      "|    total_timesteps | 256       |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| step/                   |             |\n",
      "|    rewards              | 0.7255447   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 512         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042909637 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.54       |\n",
      "|    explained_variance   | 0.00669     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.187       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 13.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| step/                   |             |\n",
      "|    rewards              | 0.76132625  |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 768         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031204216 |\n",
      "|    clip_fraction        | 0.364       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.57       |\n",
      "|    explained_variance   | -0.976      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00954    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0146      |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 9.95        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| step/                   |             |\n",
      "|    rewards              | 0.7600372   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 336         |\n",
      "|    total_timesteps      | 1024        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051722378 |\n",
      "|    clip_fraction        | 0.497       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.58       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.321       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0274      |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 8.38        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| step/                   |             |\n",
      "|    rewards              | 0.7426791   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 1280        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030275915 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.57       |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.349       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0671      |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| step/                   |            |\n",
      "|    rewards              | 0.7784008  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3          |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 504        |\n",
      "|    total_timesteps      | 1536       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09122687 |\n",
      "|    clip_fraction        | 0.472      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.57      |\n",
      "|    explained_variance   | 0.498      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.176      |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | 0.0252     |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 5.49       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| step/                   |            |\n",
      "|    rewards              | 0.72612464 |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3          |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 588        |\n",
      "|    total_timesteps      | 1792       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19989425 |\n",
      "|    clip_fraction        | 0.694      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.57      |\n",
      "|    explained_variance   | -0.114     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.274      |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | 0.0828     |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 4.16       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| step/                   |             |\n",
      "|    rewards              | 0.70441985  |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 672         |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015274599 |\n",
      "|    clip_fraction        | 0.475       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.58       |\n",
      "|    explained_variance   | -0.678      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.374       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.0434      |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.3         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| step/                   |            |\n",
      "|    rewards              | 0.7412381  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3          |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 756        |\n",
      "|    total_timesteps      | 2304       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05458318 |\n",
      "|    clip_fraction        | 0.529      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.58      |\n",
      "|    explained_variance   | -0.159     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.527      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | 0.0568     |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 3.51       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| step/                   |            |\n",
      "|    rewards              | 0.7419391  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3          |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 840        |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14825705 |\n",
      "|    clip_fraction        | 0.562      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.58      |\n",
      "|    explained_variance   | 0.224      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.604      |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | 0.0536     |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 9.46       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| step/                   |           |\n",
      "|    rewards              | 0.7737292 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 3         |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 924       |\n",
      "|    total_timesteps      | 2816      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 7.0730715 |\n",
      "|    clip_fraction        | 0.846     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.58     |\n",
      "|    explained_variance   | 0.65      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.464     |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | 0.215     |\n",
      "|    std                  | 1.01      |\n",
      "|    value_loss           | 2.48      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| step/                   |            |\n",
      "|    rewards              | 0.74018323 |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3          |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 1008       |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26635465 |\n",
      "|    clip_fraction        | 0.717      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.59      |\n",
      "|    explained_variance   | 0.778      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.206      |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | 0.098      |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 2.34       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| step/                   |           |\n",
      "|    rewards              | 0.7574647 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 3         |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 1092      |\n",
      "|    total_timesteps      | 3328      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1520372 |\n",
      "|    clip_fraction        | 0.695     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.6      |\n",
      "|    explained_variance   | 0.737     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.04      |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | 0.0909    |\n",
      "|    std                  | 1.02      |\n",
      "|    value_loss           | 4.49      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| step/                   |            |\n",
      "|    rewards              | 0.71666765 |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3          |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 1176       |\n",
      "|    total_timesteps      | 3584       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 1.6827309  |\n",
      "|    clip_fraction        | 0.912      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.61      |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.47       |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | 0.207      |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 7.24       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| step/                   |           |\n",
      "|    rewards              | 0.7335616 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 3         |\n",
      "|    iterations           | 15        |\n",
      "|    time_elapsed         | 1260      |\n",
      "|    total_timesteps      | 3840      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0313431 |\n",
      "|    clip_fraction        | 0.778     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.61     |\n",
      "|    explained_variance   | 0.439     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.341     |\n",
      "|    n_updates            | 140       |\n",
      "|    policy_gradient_loss | 0.153     |\n",
      "|    std                  | 1.02      |\n",
      "|    value_loss           | 1.47      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| step/                   |            |\n",
      "|    rewards              | 0.72054565 |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3          |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 1344       |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.83231163 |\n",
      "|    clip_fraction        | 0.777      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.61      |\n",
      "|    explained_variance   | 0.696      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.957      |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | 0.164      |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 2          |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| step/                   |            |\n",
      "|    rewards              | 0.708792   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3          |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 1428       |\n",
      "|    total_timesteps      | 4352       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10108198 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.63      |\n",
      "|    explained_variance   | 0.668      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.369      |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | 0.109      |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 2.27       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| step/                   |           |\n",
      "|    rewards              | 0.7536908 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 3         |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 1512      |\n",
      "|    total_timesteps      | 4608      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9944101 |\n",
      "|    clip_fraction        | 0.926     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.63     |\n",
      "|    explained_variance   | 0.954     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.82      |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | 0.242     |\n",
      "|    std                  | 1.02      |\n",
      "|    value_loss           | 1.21      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| step/                   |           |\n",
      "|    rewards              | 0.7474042 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 3         |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 1596      |\n",
      "|    total_timesteps      | 4864      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.12878   |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.64     |\n",
      "|    explained_variance   | -1.17     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.073     |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | 0.0989    |\n",
      "|    std                  | 1.02      |\n",
      "|    value_loss           | 2.39      |\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check_env(env)\n",
    "# # 使用PPO算法进行强化学习\n",
    "model = PPO(\n",
    "    ActorCriticCnnPolicy, env, verbose=1, device=device,\n",
    "    n_steps=256, batch_size=32,\n",
    "    tensorboard_log=\"/home/hou63/pj2/Nematic_RL/logs\")\n",
    "\n",
    "tic = time.time()\n",
    "model.learn(total_timesteps=16000, callback=RewardLoggingCallback())\n",
    "toc = time.time()\n",
    "print('Time cost: ', toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(\"/home/hou63/pj2/Nematic_RL/models/ppo_nematic\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
