{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorboard\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.model import DownSampleConv, UpSampleConv\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3, 257)\n",
      "(256, 256, 217)\n",
      "(217, 256, 256)\n",
      "(217, 2, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "D = sio.loadmat('/home/hou63/pj2/Nematic_RL/datas/D.mat')['D']\n",
    "# print keys\n",
    "print(D.shape)\n",
    "d11 = D[:,:,0,40:]\n",
    "d12 = D[:,:,1,40:]\n",
    "print(d11.shape)\n",
    "# put axis 2 to 0\n",
    "d11 = np.moveaxis(d11, 2, 0)\n",
    "d12 = np.moveaxis(d12, 2, 0)\n",
    "print(d11.shape)\n",
    "ds = np.stack((d11, d12), axis=1)\n",
    "print(ds.shape)\n",
    "ds = torch.tensor(ds, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot D\n",
    "plt.figure()\n",
    "plt.imshow(d11[:,:,0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = DownSampleConv()\n",
    "decoder = UpSampleConv()\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "\n",
    "# 训练参数\n",
    "num_epochs = 10\n",
    "dataset = TensorDataset(ds)  # 将 ds 封装为 dataset\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.2509\n",
      "Model saved with loss 0.2509\n",
      "Epoch [2/200], Loss: 0.2440\n",
      "Model saved with loss 0.2440\n",
      "Epoch [3/200], Loss: 0.2394\n",
      "Model saved with loss 0.2394\n",
      "Epoch [4/200], Loss: 0.2351\n",
      "Model saved with loss 0.2351\n",
      "Epoch [5/200], Loss: 0.2319\n",
      "Model saved with loss 0.2319\n",
      "Epoch [6/200], Loss: 0.2301\n",
      "Model saved with loss 0.2301\n",
      "Epoch [7/200], Loss: 0.2295\n",
      "Model saved with loss 0.2295\n",
      "Epoch [8/200], Loss: 0.2287\n",
      "Model saved with loss 0.2287\n",
      "Epoch [9/200], Loss: 0.2282\n",
      "Model saved with loss 0.2282\n",
      "Epoch [10/200], Loss: 0.2276\n",
      "Model saved with loss 0.2276\n",
      "Epoch [11/200], Loss: 0.2272\n",
      "Model saved with loss 0.2272\n",
      "Epoch [12/200], Loss: 0.2267\n",
      "Model saved with loss 0.2267\n",
      "Epoch [13/200], Loss: 0.2265\n",
      "Model saved with loss 0.2265\n",
      "Epoch [14/200], Loss: 0.2260\n",
      "Model saved with loss 0.2260\n",
      "Epoch [15/200], Loss: 0.2257\n",
      "Model saved with loss 0.2257\n",
      "Epoch [16/200], Loss: 0.2251\n",
      "Model saved with loss 0.2251\n",
      "Epoch [17/200], Loss: 0.2250\n",
      "Model saved with loss 0.2250\n",
      "Epoch [18/200], Loss: 0.2249\n",
      "Model saved with loss 0.2249\n",
      "Epoch [19/200], Loss: 0.2247\n",
      "Model saved with loss 0.2247\n",
      "Epoch [20/200], Loss: 0.2243\n",
      "Model saved with loss 0.2243\n",
      "Epoch [21/200], Loss: 0.2240\n",
      "Model saved with loss 0.2240\n",
      "Epoch [22/200], Loss: 0.2241\n",
      "Epoch [23/200], Loss: 0.2238\n",
      "Model saved with loss 0.2238\n",
      "Epoch [24/200], Loss: 0.2235\n",
      "Model saved with loss 0.2235\n",
      "Epoch [25/200], Loss: 0.2235\n",
      "Model saved with loss 0.2235\n",
      "Epoch [26/200], Loss: 0.2234\n",
      "Model saved with loss 0.2234\n",
      "Epoch [27/200], Loss: 0.2234\n",
      "Model saved with loss 0.2234\n",
      "Epoch [28/200], Loss: 0.2228\n",
      "Model saved with loss 0.2228\n",
      "Epoch [29/200], Loss: 0.2230\n",
      "Epoch [30/200], Loss: 0.2228\n",
      "Model saved with loss 0.2228\n",
      "Epoch [31/200], Loss: 0.2230\n",
      "Epoch [32/200], Loss: 0.2226\n",
      "Model saved with loss 0.2226\n",
      "Epoch [33/200], Loss: 0.2223\n",
      "Model saved with loss 0.2223\n",
      "Epoch [34/200], Loss: 0.2221\n",
      "Model saved with loss 0.2221\n",
      "Epoch [35/200], Loss: 0.2223\n",
      "Epoch [36/200], Loss: 0.2223\n",
      "Epoch [37/200], Loss: 0.2221\n",
      "Epoch [38/200], Loss: 0.2220\n",
      "Model saved with loss 0.2220\n",
      "Epoch [39/200], Loss: 0.2220\n",
      "Epoch [40/200], Loss: 0.2221\n",
      "Epoch [41/200], Loss: 0.2218\n",
      "Model saved with loss 0.2218\n",
      "Epoch [42/200], Loss: 0.2218\n",
      "Epoch [43/200], Loss: 0.2218\n",
      "Epoch [44/200], Loss: 0.2219\n",
      "Epoch [45/200], Loss: 0.2217\n",
      "Model saved with loss 0.2217\n",
      "Epoch [46/200], Loss: 0.2217\n",
      "Epoch [47/200], Loss: 0.2215\n",
      "Model saved with loss 0.2215\n",
      "Epoch [48/200], Loss: 0.2217\n",
      "Epoch [49/200], Loss: 0.2216\n",
      "Epoch [50/200], Loss: 0.2213\n",
      "Model saved with loss 0.2213\n",
      "Epoch [51/200], Loss: 0.2212\n",
      "Model saved with loss 0.2212\n",
      "Epoch [52/200], Loss: 0.2213\n",
      "Epoch [53/200], Loss: 0.2215\n",
      "Epoch [54/200], Loss: 0.2212\n",
      "Epoch [55/200], Loss: 0.2212\n",
      "Model saved with loss 0.2212\n",
      "Epoch [56/200], Loss: 0.2212\n",
      "Epoch [57/200], Loss: 0.2212\n",
      "Epoch [58/200], Loss: 0.2210\n",
      "Model saved with loss 0.2210\n",
      "Epoch [59/200], Loss: 0.2210\n",
      "Epoch [60/200], Loss: 0.2211\n",
      "Epoch [61/200], Loss: 0.2211\n",
      "Epoch [62/200], Loss: 0.2209\n",
      "Model saved with loss 0.2209\n",
      "Epoch [63/200], Loss: 0.2208\n",
      "Model saved with loss 0.2208\n",
      "Epoch [64/200], Loss: 0.2209\n",
      "Epoch [65/200], Loss: 0.2208\n",
      "Epoch [66/200], Loss: 0.2209\n",
      "Epoch [67/200], Loss: 0.2209\n",
      "Epoch [68/200], Loss: 0.2208\n",
      "Epoch [69/200], Loss: 0.2206\n",
      "Model saved with loss 0.2206\n",
      "Epoch [70/200], Loss: 0.2207\n",
      "Epoch [71/200], Loss: 0.2209\n",
      "Epoch [72/200], Loss: 0.2205\n",
      "Model saved with loss 0.2205\n",
      "Epoch [73/200], Loss: 0.2206\n",
      "Epoch [74/200], Loss: 0.2207\n",
      "Epoch [75/200], Loss: 0.2209\n",
      "Epoch [76/200], Loss: 0.2205\n",
      "Model saved with loss 0.2205\n",
      "Epoch [77/200], Loss: 0.2206\n",
      "Epoch [78/200], Loss: 0.2206\n",
      "Epoch [79/200], Loss: 0.2207\n",
      "Epoch [80/200], Loss: 0.2208\n",
      "Epoch [81/200], Loss: 0.2203\n",
      "Model saved with loss 0.2203\n",
      "Epoch [82/200], Loss: 0.2205\n",
      "Epoch [83/200], Loss: 0.2204\n",
      "Epoch [84/200], Loss: 0.2203\n",
      "Model saved with loss 0.2203\n",
      "Epoch [85/200], Loss: 0.2203\n",
      "Epoch [86/200], Loss: 0.2203\n",
      "Epoch [87/200], Loss: 0.2203\n",
      "Epoch [88/200], Loss: 0.2203\n",
      "Epoch [89/200], Loss: 0.2203\n",
      "Epoch [90/200], Loss: 0.2202\n",
      "Model saved with loss 0.2202\n",
      "Epoch [91/200], Loss: 0.2201\n",
      "Model saved with loss 0.2201\n",
      "Epoch [92/200], Loss: 0.2204\n",
      "Epoch [93/200], Loss: 0.2205\n",
      "Epoch [94/200], Loss: 0.2204\n",
      "Epoch [95/200], Loss: 0.2204\n",
      "Epoch [96/200], Loss: 0.2204\n",
      "Epoch [97/200], Loss: 0.2203\n",
      "Epoch [98/200], Loss: 0.2203\n",
      "Epoch [99/200], Loss: 0.2203\n",
      "Epoch [100/200], Loss: 0.2203\n",
      "Epoch [101/200], Loss: 0.2202\n",
      "Epoch [102/200], Loss: 0.2202\n",
      "Epoch [103/200], Loss: 0.2203\n",
      "Epoch [104/200], Loss: 0.2200\n",
      "Model saved with loss 0.2200\n",
      "Epoch [105/200], Loss: 0.2200\n",
      "Epoch [106/200], Loss: 0.2201\n",
      "Epoch [107/200], Loss: 0.2202\n",
      "Epoch [108/200], Loss: 0.2201\n",
      "Epoch [109/200], Loss: 0.2201\n",
      "Epoch [110/200], Loss: 0.2201\n",
      "Epoch [111/200], Loss: 0.2200\n",
      "Epoch [112/200], Loss: 0.2202\n",
      "Epoch [113/200], Loss: 0.2201\n",
      "Epoch [114/200], Loss: 0.2200\n",
      "Epoch [115/200], Loss: 0.2203\n",
      "Epoch [116/200], Loss: 0.2203\n",
      "Epoch [117/200], Loss: 0.2200\n",
      "Model saved with loss 0.2200\n",
      "Epoch [118/200], Loss: 0.2201\n",
      "Epoch [119/200], Loss: 0.2200\n",
      "Model saved with loss 0.2200\n",
      "Epoch [120/200], Loss: 0.2202\n",
      "Epoch [121/200], Loss: 0.2200\n",
      "Epoch [122/200], Loss: 0.2199\n",
      "Model saved with loss 0.2199\n",
      "Epoch [123/200], Loss: 0.2198\n",
      "Model saved with loss 0.2198\n",
      "Epoch [124/200], Loss: 0.2199\n",
      "Epoch [125/200], Loss: 0.2201\n",
      "Epoch [126/200], Loss: 0.2198\n",
      "Epoch [127/200], Loss: 0.2200\n",
      "Epoch [128/200], Loss: 0.2200\n",
      "Epoch [129/200], Loss: 0.2198\n",
      "Epoch [130/200], Loss: 0.2198\n",
      "Epoch [131/200], Loss: 0.2198\n",
      "Epoch [132/200], Loss: 0.2199\n",
      "Epoch [133/200], Loss: 0.2199\n",
      "Epoch [134/200], Loss: 0.2199\n",
      "Epoch [135/200], Loss: 0.2202\n",
      "Epoch [136/200], Loss: 0.2198\n",
      "Epoch [137/200], Loss: 0.2198\n",
      "Epoch [138/200], Loss: 0.2199\n",
      "Epoch [139/200], Loss: 0.2200\n",
      "Epoch [140/200], Loss: 0.2200\n",
      "Epoch [141/200], Loss: 0.2199\n",
      "Epoch [142/200], Loss: 0.2200\n",
      "Epoch [143/200], Loss: 0.2198\n",
      "Epoch [144/200], Loss: 0.2199\n",
      "Epoch [145/200], Loss: 0.2200\n",
      "Epoch [146/200], Loss: 0.2199\n",
      "Epoch [147/200], Loss: 0.2198\n",
      "Epoch [148/200], Loss: 0.2199\n",
      "Epoch [149/200], Loss: 0.2198\n",
      "Epoch [150/200], Loss: 0.2198\n",
      "Epoch [151/200], Loss: 0.2198\n",
      "Model saved with loss 0.2198\n",
      "Epoch [152/200], Loss: 0.2198\n",
      "Epoch [153/200], Loss: 0.2199\n",
      "Epoch [154/200], Loss: 0.2197\n",
      "Model saved with loss 0.2197\n",
      "Epoch [155/200], Loss: 0.2199\n",
      "Epoch [156/200], Loss: 0.2198\n",
      "Epoch [157/200], Loss: 0.2198\n",
      "Epoch [158/200], Loss: 0.2200\n",
      "Epoch [159/200], Loss: 0.2198\n",
      "Epoch [160/200], Loss: 0.2199\n",
      "Epoch [161/200], Loss: 0.2198\n",
      "Epoch [162/200], Loss: 0.2199\n",
      "Epoch [163/200], Loss: 0.2198\n",
      "Epoch [164/200], Loss: 0.2198\n",
      "Epoch [165/200], Loss: 0.2197\n",
      "Model saved with loss 0.2197\n",
      "Epoch [166/200], Loss: 0.2199\n",
      "Epoch [167/200], Loss: 0.2196\n",
      "Model saved with loss 0.2196\n",
      "Epoch [168/200], Loss: 0.2199\n",
      "Epoch [169/200], Loss: 0.2198\n",
      "Epoch [170/200], Loss: 0.2198\n",
      "Epoch [171/200], Loss: 0.2197\n",
      "Epoch [172/200], Loss: 0.2196\n",
      "Model saved with loss 0.2196\n",
      "Epoch [173/200], Loss: 0.2198\n",
      "Epoch [174/200], Loss: 0.2197\n",
      "Epoch [175/200], Loss: 0.2198\n",
      "Epoch [176/200], Loss: 0.2197\n",
      "Epoch [177/200], Loss: 0.2193\n",
      "Model saved with loss 0.2193\n",
      "Epoch [178/200], Loss: 0.2197\n",
      "Epoch [179/200], Loss: 0.2197\n",
      "Epoch [180/200], Loss: 0.2195\n",
      "Epoch [181/200], Loss: 0.2195\n",
      "Epoch [182/200], Loss: 0.2195\n",
      "Epoch [183/200], Loss: 0.2195\n",
      "Epoch [184/200], Loss: 0.2198\n",
      "Epoch [185/200], Loss: 0.2195\n",
      "Epoch [186/200], Loss: 0.2195\n",
      "Epoch [187/200], Loss: 0.2197\n",
      "Epoch [188/200], Loss: 0.2195\n",
      "Epoch [189/200], Loss: 0.2195\n",
      "Epoch [190/200], Loss: 0.2196\n",
      "Epoch [191/200], Loss: 0.2195\n",
      "Epoch [192/200], Loss: 0.2198\n",
      "Epoch [193/200], Loss: 0.2195\n",
      "Epoch [194/200], Loss: 0.2195\n",
      "Epoch [195/200], Loss: 0.2197\n",
      "Epoch [196/200], Loss: 0.2197\n",
      "Epoch [197/200], Loss: 0.2196\n",
      "Epoch [198/200], Loss: 0.2195\n",
      "Epoch [199/200], Loss: 0.2195\n",
      "Epoch [200/200], Loss: 0.2195\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "save_path = '/home/hou63/pj2/Nematic_RL/log_model/encoder_checkpoint.pth'  # 模型保存路径\n",
    "best_loss = float('inf')  # 初始最优损失值设为无穷大\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()  # 设置 encoder 为训练模式\n",
    "    decoder.train()  # 设置 decoder 为训练模式\n",
    "    \n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data in dataloader:\n",
    "        # 将数据移到设备上\n",
    "        x = data[0].to(device)\n",
    "        \n",
    "        # 前向传播: encoder 和 decoder\n",
    "        encoded = encoder(x)\n",
    "        reconstructed = decoder(encoded)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(reconstructed, x)  # 自监督损失：重建误差\n",
    "        \n",
    "        # 反向传播与优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # 保存模型：当当前损失小于最佳损失时，保存模型\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(encoder.state_dict(), save_path)\n",
    "        print(f'Model saved with loss {best_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
